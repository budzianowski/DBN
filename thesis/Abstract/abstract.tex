% ************************** Thesis Abstract *****************************
% Use `abstract' as an option in the document class to print only the titlepage and the abstract.
\begin{abstract}
Variational approximation to the untractable free energy using high-temperature perturbation expansion, originally developed by Yedida and Georges \cite{yedidia2001idiosyncratic}, was shown to bring significant improvements over the naive mean field approach in training restricted Boltzmann machines after an appropriate adaptation \cite{gabrie2015training}. RBM can serve as a general approximator of any distribution over binary variables as well as a building block in pre-training deep multi-layer neural networks. In this thesis I replicate the main results from the work of Gabri{\'e} et al. Then, I propose a generalization of this deterministic procedure for pre-training deep belief nets and demonstrate that this algorithm provides performance equal to persistent contrastive divergence. Such pre-trained deep structures learn better lower-dimensional codes of digits from the MNIST data set than principal component analysis or shallow structures. Moreover, I compare three schedules of updates showing that the analysed method is robust to different schemes as long as the updates are performed layer-wise. Furthermore, the approximation is compared with annealed importance sampling to assess the quality of estimates of the free energy however the analysed method seems to be rather biased. Nevertheless, the results suggests that investigated deterministic approximation of the partition function might be used as a ballpark estimate during training as it is computationally-wise very inexpensive method.
% The experiments were conducted using the well-known MNIST data set.
Further development are suggested as it should be possible to generalize this approach to more general family of distributions with real-valued variables. 
Connect with EP?
\end{abstract}
