\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
Statistical analysis and inference of multivariate phenomena might be performed in the context of probabilistic graphical models that provides a natural framework for developing complicated structures where the graph of the model defines the conditional dependence between random variables. On the other hand we are interested in the class of distributions that have suitable probabilistic properties -- this is naturally enforced in Markov random fields which by definition satisfy Markov property. A strict connection between Markov random fields and graphical models that factorize enables us to analyse energy based distributions, a fundamental class of probability distributions extensively used in statistical physics, in the framework of graphical models.

A particular example of an undirected graphical model is Boltzmann machine -- structure that has been extensively investigated over last $30$ years as it can be seen as a natural approximator of any probabilistic distribution. A special class from this family is restricted Boltzmann machine (RBM) -- an undirected neural network that doesn't have connections between neighbouring nodes. It is believed that one of the main reasons for the resurgence of deep architectures is the effective unsupervised pre-training of those structures with RBMs. The advances in training deep neural networks over couple last years allowed us to train very powerful statistical models that yield substantial improvements in many areas of machine learning like speech or vision recognition, image processing or machine translation. 

One of the main reasons for the development of deep methods in the last years was introduction of effective and computationally efficient stochastic algorithms for unsupervised training. The breakthrough came in $2006$ with the contrastive divergence (CD) algorithm for training deep belief networks (DBN) \cite{hinton2006reducing}. The deep structure might be formed by stacking single RBMs pre-trained using contrastive divergence approach. It was empirically shown that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set \cite{erhan2010does}. 

However, an alternative deterministic procedure was proposed by Gabri{\'e} et al. \cite{gabrie2015training} based on the extended mean field approach originally derived in statistical physics \cite{yedidia2001idiosyncratic}. Initially, there were many attempts to train RBMs using the deterministic mean field approach were we assume that the approximate distribution factorizes. The obtained results showed that such assumption is too strong and this procedure provided poor performance when compared to CD. However, variational approximation to the untractable free energy using high-temperature perturbation expansions brings significant improvements over the naive mean field approach applied to bipartite structures, specifically to the restricted Boltzmann machine. 

The thesis is organized as follows. In chapter $1$ I will show strict mapping between Markov random fields and energy based models and embed them into the framework of probabilistic graphical models. Then, two variational approximations of the partition function will be described -- naive and extend mean field approximations. Finally, Boltzmann distribution is introduced with analysis of main properties. Chapter $2$ describes the experimental toy framework used to evaluate the performance of the extended mean field approximation and compares it to the naive approximation. Three schedules of updates are considered for magnetizations of spins in a system -- asynchronous, sequential and parallel. Two toy models are used in experiments -- grid toy model with periodic boundary conditions and a restricted Boltzmann machine with $10$ units per layer. 
The maximum likelihood training of restricted Boltzmann machine is derived in chapter $3$. Monte Carlo methods are introduced, namely Gibbs sampling. Then, the contrastive divergence procedure is shown and compared to the deterministic approach based on the extended mean field approximation. I replicate main results obtained by Gabri\'e et al. -- comparison of stochastic and deterministic trainings of restricted Boltzmann machines using log-likelihood estimates and generated samples from trained models.
In chapter $4$ three proposed schedules of updates are evaluated on the real data set. Then, I compare two methods of estimating the free energy of the model -- the annealed importance sampling and approximation based on the extended mean field approach.
Furthermore, deep belief nets are introduced and greedily training of those structures is reformulated using entirely deterministic approach. Stochastic and deterministic procedures are compared by the analysis of reconstructions of samples from data.

A detailed derivation of the extended mean field approximation is shown in Appendix A.
TOD) - mention unification?