\chapter{Introduction}
TODO inference on over?
Statistical analysis and inference on multivariate phenomena might be performed in the context of probabilistic graphical models that provides a natural framework for developing complicated structures where the graph of the model defines the conditional dependence between random variables. On the other hand we are interested in the class of distributions that have suitable probabilistic properties -- this is naturally enforced in Markov random fields which by definition satisfy Markov property. A strict connection between Markov random fields and graphical models that factorize enables us to analyse energy based models, a fundamental class of probability distributions extensively used in statistical physics, in the framework of graphical models.

A particular example of an undirected graphical model is Boltzmann machine -- structure that has been extensively investigated over last $30$ years as it can be seen as a natural approximator of any probabilistic disitribution. A special class from this family is restricted Boltzmann machine -- an undirected neural network that doesn't have connections between neighbour nodes. It is believed that one of the main reasons of the resurgence of deep architectures is the effective unsupervised pre-training of those structures with restricted Boltzmann machines. The advances in training deep neural networks over couple last years allowed us to train very powerful statistical model which yields substantial improvements in many areas of machine learning like speech or vision recognition, image processing or machine translation. 

One of the main reasons for the development of deep methods in last years is the effective and computationally efficient stochastic algorithms, namely contrastive divergence for unsupervised trainingThe breakthrough that lead to effective training strategies for deep architectures came in 2006 with the CD algorithm for training deep belief networks (DBN) \cite{hinton2006reducing}. The deep structure might be formed by stacking single RBMs pre-trained using contrastive divergence approach. It was empirically shown that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set \ref{erhan2010does}. Thus, such 

A new algorithm - deterministic - previous didn't work.\cite{gabrie2015training}

The thesis is organized as follows. In chapter $1$ I will show strict mapping between Markov random fields and energy based models and embed them into the framework of probabilistic graphical models. Then, two variational approximations of the partition function will be described -- naive and extend mean field approximation. Three schedules of updates are considered for magnetizations of spins in a system -- asynchronous, sequential and parallel. 

Chapter $2$ describes the experimental toy framework used to evaluate the performance of the extended mean field approximation and compares it to the naive approximation. Two toy models are used in experiments -- grid toy model with periodic boundary conditions and a restricted Boltzmann machine with $10$ units per layer. 

The maximum likelihood training of restricted Boltzmann machine is derived in chapter $3$. Monte Carlo methods are introduced, namely Gibbs sampling. Then, contrastive divergence procedure is shown and compared to deterministic approach based on the extended mean field approxiation. I replicate main results from the Gabri\'e et al. paper -- the evaluation of stochastic and deterministic training of restricted Boltzmann machines using log-likelihood estimates and generated samples from trained models.

In chapter $4$ three proposed schedules of updates are compared on the real data set. Then, I compare two methods of estimating the free energy of the model - the annealed importance sampling method and approximation based on the extended mean field approach.
Furthermore, deep belief nets are introduced and greedily training of those structures is reformulated using entirely deterministic approach. Stochastic and deterministic procedures are compared by analysis of reconstruction of samples from data.

A detailed derivation of the extended mean field approximation is shown in Appendix A.
