\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusions}
Variational approximation to the untractable free energy using high-temperature perturbation expansion brings significant improvements over the naive mean field approach applied to bipartite structures, specifically to the restricted Boltzmann machine. Even though the expansion is performed around the point where all spins are independent, the radius of convergence is large enough to obtain robust results for smaller temperatures. By adding higher-order terms from a systematic expansion, we are able to model dependence between variables possibly at arbitrarily order.

Experiments on toy models confirmed first promising results on the real data set shown by Gabri{\'e} et al. \cite{gabrie2015training}. Additional terms responsible for couplings and triplets of units greatly improve training efficiency and performance over the naive mean field approach in the case of the RBM structure. Those improvements can be observed even in the case of a very small model. Remarkably, for the grid model the extended approach performs worse than the naive one which might be explained by the fact that the averaging implicitly assumed in the latter case is consistent with the structure of the grid. Moreover, I analysed three different schedules of updates of magnetizations -- the results suggest that the EMF method is rather robust to the different schemes as long as the updates are performed layer-wise. Interestingly, in that case parallel updates yields similar results as an asynchronous scheme.

The quality of estimates and learned generative models is retained in the case of the real data set. The extended mean field approximation provides performance comparable and sometimes superior to contrastive divergence procedure -- the most widely used procedure for pre-training RBMs and DBNs. The experiments confirmed the previous observations that even with only $3$ iterations of magnetizations we are able to train generative models that produce samples of good quality. This shows that the EMF method might be used when fast deterministic training is desired.

The experiments performed on the real data set yields results that are in accordance to the ones obtained on toy models. No differences were observed between parallel and asynchronous updates which suggests that this method might be applied to more complicated graphs' structures. Unsatisfactorily the comparison with annealed importance sampling shows that the EMF approximation of the partition function is strongly biased even with a small number of hidden units. However, the computational cost is significantly lower and  this method might be used if we need a ballpark figure of the value of the partition function during training. Moreover, the AIS procedure needs to be adapted to specific structure of the model while the EMF approximation can be used with any energy based model defined over binary variables.

The natural generalization to training deep structures was suggested where deep belief nets are pre-trained using greedy layer-wise deterministic procedure based on the EMF approximation. The hierarchical structure enables to process information through multiple stages of transformation which results in better reconstructions than shallow structures. Results also suggest that this procedure allows to initialize weights effectively for supervised training later on. As it was the case for the single RBM, the deterministic algorithm achieves comparable performance to stochastic procedure based on the contrastive divergence algorithm.

OPPER  mannfredintro to general case.TODO possibsdsly to extend the approach to arbitrary order.

The code needed to replicate all results is available at \url{https://github.com/budzianowski/DRBM}. 

