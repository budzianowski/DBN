\documentclass[../report/report.tex]{subfiles}

\begin{document}

\subsection{Graphical models as Markov Random Fields}
One of the basic concepts in statistical modelling are graphical models which greatly help in analysing multivariate phenomenon. Visualization by graphs help with efficient development and understanding analysed models while complex computations can be performed Consider a graph $G = (V,E)$ which consists of a finite set of vertices $V$ and a collection of edges $E \subset V \times V$. Each edge $e_i \in E$ joins two vertices and in general may have a direction. The vertex $v \in V$ may be seen as a random variable $X_v$ defined on some space $\mathcal{X}_v$ that may be either continuous or discrete. We will use the notion of \emph{clique} which is a subset of $V$ in which all nodes are pairwise connected. One of the most useful family of models are Markov Random Fields which are type undirected random fields which satisfies global Markov property, specifically:
\begin{mydef} \cite{koller2009probabilistic} An undirected graphical model G is a Markov Random Field (MRF) if for any node $X_v$ in the graph the following conditional property holds:
\begin{align*}
\begin{split}
P(X_i |X_{G\backslash i} ) = P(X_i | X_{N(i)})
\end{split}
\end{align*}
where $X_{G\backslash i}$ denotes all the nodes except $X_i$, and $X_{N(i)}$ denotes the set of all vertices connected to $X_i$.
\end{mydef}
Thus, the MRF has a desired property that any two nodes are conditionally independent given some evidence nodes that separates them. This property is closely related with the notion of factorization of the joint probability distribution:
\begin{mydef} A probability distribution $P(athbf{X})$ on an undirected graphical model $G$ factorizes over $G$ if there exists a set of non-negative functions (potentials) $\{ \psi_{C} \}_{C \in \mathcal{C}}$ functions on cliques that cover all the nodes and edges of $G$ and we can write:
\begin{align*}
\begin{split}
P(X_1, X_2, ..., X_n) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi
_C(x_C)
\end{split}
\end{align*}
where $\mathcal{C}$ is a set of all cliques in $G$ and $Z$ is a normalization constant $Z = \sum_{\mathbf{x}} \prod_{C \in \mathcal{C}} \psi_C(X_C) $ which is often called a partition function.
\end{mydef}

\begin{theorem} [Hammersley-Clifford, 1972?] Strictly positive distribution $P(\mathbf{X})$ is MRF  w.r.t an undirected graph $G$ if and only if it factorizes over $G$.
\end{theorem}
This theorem ensures us that there exits a general factorization form of the distribution of MRFs. It follows from the strict positivity of $P$ that we can write:
\begin{align*}
\begin{split}
p(x_1, x_2, ..., x_n) = \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi
_C(x_C) = \frac{1}{Z} e ^{\sum_{C \in \mathcal{C}} \ln \psi
_C(x_C) } = \frac{1}{Z} e ^{-E(x)}
\end{split}
\label{eq:gibbsDistribution}
\end{align*}
where $E(x)$ is called an energy function. This general form of distribution is usually defined as \emph{Gibbs distribution}. Hence, the probability distribution of every MRF can be expressed as in \ref{eq:gibbsDistribution}. 
This relationship allow us to take advantage of  Moreover, this form of distribution is a natural candidate to approximate and model phenomenon which can be seen as graphical models. In next sections we will analyse one of examples of Gibbs which will is powerful enough to approximate any probability distribution (TODO  - find concept of apprximation by BM)
%$\mathbf{\theta}$ - we will not write explicitly the parameters.

\subsection{Boltzmann distribution}
In this thesis, a primary undirected graphical model (or MRF) which will be analysed in the general form has the joint distribution:
$$p(x_1,x_2, ..., x_n) = \frac{1}{Z}\exp(-E(x_1,x_2, ..., x_n)$$
where $E$ is the \emph{energy} of the system of the form:
$$E(\mathbf{X}) = -\sum_{(ij)} w_{ij} x_i x_j -\sum_i  \theta_i x_i$$
The pair-wise potential function have here the form:
$$\psi_{i,j} = \exp(x_i w_{ij} x_j)$$
while the magnetic fields defined as:
$$\psi_i = \exp(a_i x_i)$$
The name comes from the Boltzmann distribution which extensively used in physics to compute the energy of the system of particles. This model proves to be very useful for many applications such as the error-correcting code, computer vision, medical diagnosis or statistical mechanics \cite{yedidia2001idiosyncratic}. This model may represents the statistical dependencies between different variables through the weight link $w_{ij}$ as well as the evidence for the specific variable. However, computing the partition function requires to sum over a number of states that grows exponentially with the number of variables and is untractable even for a small number of variables. That is why, we have to resort to some tractable approximation which one of them will be analysed in next section.
\subsection{Statistical Perspective}
Consider energy based model (initially without latent variables) of the form:
\begin{equation}
P(\mathbf{s}) = \frac{ e^{-E(\mathbf{s})}}{\sum_{\mathbf{s}} e^{-E(\mathbf{s})}} = \frac{1}{Z}e^{-E(\mathbf{s})}
\label{eq:distribution}
\end{equation}
where energy is defined as:
$$E \equiv E (\mathbf{s}) = - \sum_{(ij)} s_i w_{ij} s_j - \sum_i a_i s_i $$
If we restrict each node to have two states, $s_i \in \{0,1 \}$, we obtain well-known the Ising model \cite{yedidia2001idiosyncratic}. Restricting the $w_{ij}$ to be positive we obtain the ferromagnetic Ising model. Finally, assuming that the $w_{ij}$ are chosen from a random distribution, we obtain the Ising spin glass model. Instead of imposing some restrictions on model structure we will try to find a approximate distribution $Q$ (which poses useful characteristics) that minimizes relative entropy often called Kullback-Leibler divergence:
\begin{equation}
KL(Q || P)  = \mathbb{E}_{Q}\left( \ln \frac{Q}{P} \right) = \sum_{\mathbf{s}} Q(\mathbf{s}) \ln \frac{Q(\mathbf{s})}{P(\mathbf{s})}  
\label{eq:kl}
\end{equation}
The $KL$-divergence is non-symmetric measure of the difference between two distributions which is always non-negative. Substituting $P$ from \ref{eq:distribution} into previous equation gives us:
$$ KL(Q||P) = \ln Z + E[Q] - H[Q]$$
where $H$ stands for entropy of the distribution $Q$, $\ln Z$ is the \emph{free energy} and $E[Q] = \sum_{\mathbf{s}} Q(\mathbf{s})E(\mathbf{s})$ is called the \emph{variational energy} \cite{opper2001advanced}. The partition function $Z$ doesn't depend on $Q$ and we need to only focus on minimizing the variational free energy:
\begin{align}
\begin{split}
F[Q] = E[Q] - H[Q].
\label{eq:gibbsFreeEnergy}
\end{split}
\end{align}

% On the other hand we have (TODO: derive):
%$$\ln P(\mathbf{s}) = KL(Q || P) + F(Q,\mathbf{s})
%\label{eq:kl}
%$$
%Thus, minimizing KL-term is equivalent to maximizing $F$ which is commonly known as a free energy.


\subsection{Mean Field Approximation}
\subsection{Extended Mean Field Approximation}
Mean field approach (TODO: murphy here why MF):
\begin{align}
\begin{split}
Q(\mathbf{s}) = \prod_i q_i(x_i)
\end{split}
\end{align}
MF theory is exact for infinite-ranged Ising model. In the case of spin glass model  Thouless, Anderson and Palmer (TAP) showed the corresponding Gibbs free energy with additional "Onsager term".
\begin{align}
\begin{split}
\ln Z = \ln \sum_{\mathbf{s}} \exp(-E(\mathbf{s})) = \ln \sum_{\mathbf{s}} q(\mathbf{s}) \frac{ \exp(-E(\mathbf{s}))}{q(\mathbf{s})} \geqslant  \sum_{\mathbf{s}} q(\mathbf{s}) \ln  \frac{ \exp(-E(\mathbf{s}))}{q(\mathbf{s})}  = -\mathbb{E}(E(\mathbf{s})) + H(Q) = -F
\end{split}
\end{align}
The bound F on log Z is called \emph{the mean field free energy}.

We will minimize \ref{eq:gibbsFreeEnergy} where we instead of assuming $Q$ to be a product distribution we require that our distribution has to satisfy:
\begin{align}
\begin{split}
\mathbb{E}_Q (\mathbf{S}) = \mathbf{m} 
\label{eq:constraint}
\end{split}
\end{align}
 where $\mathbb{E}$ refers to the average configuration under the Boltzmann measure). Thus, the Gibbs' free energy is defined as 
 \begin{align}
\begin{split}
G(\mathbf{m}) = \min_{Q} \{E(Q) - H(Q) ~|~\mathbb{E}(\mathbf{S}) = \mathbf{m} \}
\label{eq:minimum}
\end{split}
\end{align}
The constrained optimization problem can be transformed into unconstrained using Lagrange multipliers, i.e.:
 \begin{align}
\begin{split}
E(Q) - H(Q) -  \sum_i \lambda_i(S_i - m_i) 
\end{split}
\end{align}
which leads to the new form of the free energy with auxiliary fields:
 \begin{align}
\begin{split}
F(\boldsymbol{\lambda}) = \sum_{\mathbf{s}} \exp( -E(\mathbf{s}) -\sum_i \lambda_i s_i)
\end{split}
\end{align}
Using this new free energy in the equation \ref{eq:minimum} yields:
 \begin{align}
\begin{split}
G(\mathbf{m}, \boldsymbol{\lambda} ) = \sum_i \lambda_i m_i - \ln \sum_\mathbf{s} \exp(-E(\mathbf{s}) + \sum_i \lambda_i s_i)
\end{split}
\end{align}
Through the Legendre transform we can conjugate both unknown parameters and the condition \ref{eq:constraint} on the $\lambda_i$ can be introduced.
 \begin{align}
\begin{split}
G(\mathbf{m}) = \max_{\boldsymbol{\lambda}}\{ \sum_i \lambda_i m_i - \ln \sum_\mathbf{s} \exp(-E(\mathbf{s}) + \sum_i \lambda_i s_i) \}
\end{split}
\end{align}
where now the the maximizing auxiliary field $\boldsymbol{\lambda}^*(\mathbf{m})$ is the inverse function of $\mathbf{m}(\boldsymbol{\lambda}) = \frac{\text{d}F}{\text{d} \boldsymbol{\lambda}}$.
As we operate on the concave function, the Legendre transform is its own inverse and and we can restore the free energy by setting the auxiliary field to zero which yields:
 \begin{align}
\begin{split}
F = \min_{\mathbf{m}} G(\mathbf{m}).
\end{split}
\end{align}
Now lets assume inifinite temperature. - expand there
Here Georges:
Variational free energy with a set of external auxiliary fields (Lagrange multipliers):
$$ - \beta G = \ln \sum_{\mathbf{s}} \exp \left( \beta \sum_{(ij)} w_{ij} s_i s_j +\beta \sum_i  a_i s_i  + \sum_i \lambda_i (\beta) (s_i - m_i) \right)$$
We are interested in expanding  $-\beta G(\beta, \mathbf{m})$ around $\beta =0$ where the spins are entirely controlled by their auxiliary fields:
$$-\beta G = -(\beta G)_{\beta=0} - \left( \frac{\partial (\beta G)}{\partial \beta}\right)_{\beta = 0}  \beta - \left( \frac{\partial^2 (\beta G)}{\partial \beta^2}\right)_{\beta = 0}  \frac{\beta^2}{2} - ...$$
At $\beta = 0$ the spins are uncorrelated:
\begin{align}
\begin{split}
 m_i = \mathbb{E}_{\beta =0}(s_i) = \frac{\exp(\lambda_i(0))}{\exp(\lambda_i(0)) + 1}= \text{sigmoid}(\lambda_i(0))
\label{eq:mi}
\end{split}
\end{align}
and
\begin{align}
\begin{split}
 -(\beta G)_{\beta =0 } = & \ln \sum_{\mathbf{s}} \exp \left( \sum_i \lambda_i (0) (s_i - m_i) \right) \\
 = & \ln \left\lbrace \sum_{s_1}  \exp \left( \lambda_1 (0) (s_1 - m_1) \right) ... \sum_{s_n}  \exp \left(\lambda_n (0) (s_n - m_n) \right) \right\rbrace \\
 = &\ln \left\lbrace (\exp(\lambda_i(0)) +1)\exp(-\lambda_1(0)m_1) ... (\exp(\lambda_i(0)) +1)\exp(-\lambda_1(0)m_n) \right\rbrace \\
 = & \sum_i \left( \ln \left[ 1 + \left( \frac{m_i}{1-m_i}\right) \right] - \lambda_i(0)m_i  \right)
\end{split}
\end{align}
Using \ref{eq:mi}, we obtain:
$$\lambda_i(0) = \text{logit} (m_i) = \ln \left( \frac{m_i}{1-m_i} \right)$$
and thus:
\begin{align*}
\begin{split}
 -(\beta G)_{\beta =0 } = & \sum_i \left\lbrace  \ln \left( \frac{1}{1 -m_i}  \right) - m_i\ln \left(\frac{m_i}{1- m_i} \right)   \right\rbrace\\
 = & - \sum_i \left[m_i\ln (m_i) +  (1 - m_i)\ln \left( 1-m_i \right)\right]  
 \end{split}
\end{align*}
 Yedida and Georges showed how to create the Taylor expansion beyond $O(\beta^2)$ (TODO referring to MF and TAP variant) (derivation in Appendix):
\begin{align*}
\begin{split}
-\beta G = & - \sum_i \left[m_i\ln (m_i) +  (1 - m_i)\ln \left( 1-m_i \right)\right]  \\
& + \beta \sum_{(ij)} w_{ij} m_i m_j + \beta \sum_i a_i m_i\\
& + \frac{\beta^2}{2} \sum_{(ij)} w_{ij}^2 (m_i - m_i^2)(m_j-m_j^2)\\
& + \frac{2\beta^3}{3} \sum_{(ij)} w_{ij}^3 (m_i - m_i^2)(\frac{1}{2} - m_i)(m_j - m_j^2)(\frac{1}{2} - m_j)
\label{eq:EMFexpansion}
\end{split}
\end{align*}

TODO:Add self consistency relations:

The variational free energy $\mathcal{F}$ is highly multimodal in real world examples in which case all stationary points either maximum, minimum or saddle satisfies the self-consistency relations \ref{eq:selfConsistency}.


\subsection{Marginal polytope - bounds for the partition function}
Although it is very straightforward to obtain naive mean field approach from the TAP expansions, unlike the former this method doesn't bound in any way the variational free energy. Moreover, the approximation was based on the Taylor expansion which poses a threat that the radius of convergence of the expansion will be too small to obtain robust results for the different values of $\beta$ \cite{yedidia2001idiosyncratic}.
There are a few examples in statistical physics where this method works very reliably in a wide variety of temperatures  \cite{plefka1982convergence} however in general there aren't any theoretical foundations for the robustness of the TAP expansion.


\subsection{Boltzmann Machine}
A Boltzmann machine is a network of symmetrically coupled stochastic binary units with both visible-to-visible and hidden-to-hidden lateral connections with energy function defined as:
 \cite{ackley1985learning}
\subsubsection{Approximator for any distribution}
An example of such structure presents Figure \ref{fig:BM}.

\subsection{RBMs}
Representational power - bengio -any distibution can be modelled by RBM. TODO.
$$ E (\mathbf{v,h}) = - \sum_i a_i v_i - \sum_j b_j h_j - \sum_{i,j} v_i w_{ij}h_j$$

$$ P(\mathbf{v, h}) = \frac{1}{Z} e^{-E(\mathbf{v,h})}
$$

$$ Z  =  \sum_{\mathbf{v,h}} e^{-E(\mathbf{v,h})} $$

$$ F = - \ln Z $$

$$ F^c (\mathbf{v}) = \ln \left( \sum_{\mathbf{h}} e^{-E(\mathbf{v,h})} \right)= \ln \left( \sum_{h_1} e^{-E(\mathbf{v},h_1)}  ...  \sum_{h_n} e^{-E(\mathbf{v},h_n)} \right) \\
$$

$$\mathcal{L} = \ln P(\mathbf{v}) = F^c(\mathbf{v}) - F$$
where $F$ is the \emph{free energy} of the RBM.

\subsubsection{Exploiting the RBM structure}
The restrictions imposed on the structure allows for efficient computation of the clamped free energy because the hidden variables are independent given the state of the visible variables and vice versa and we can write:
\begin{align}
\begin{split}
p(\mathbf{h}| \mathbf{v}) = \prod_{i = 1}^{m} p(h_i |\mathbf{v}) \\
p(\mathbf{v}| \mathbf{h}) = \prod_{i = 1}^{n} p(v_i |\mathbf{h})
\end{split}
\end{align}

Clamped free energy can be written in the form:
\begin{align}
\begin{split}
\mathcal{F}^c(\mathbf{v}) = & \sum_\mathbf{h} e^{-E(\mathbf{v}, \mathbf{h})} = e^{\mathbf{b}'\mathbf{v}}\sum_{h_1}...\sum_{h_m}e^{-E(\mathbf{v}, \mathbf{h})} \\
=&  e^{\mathbf{b}'\mathbf{v}} \sum_{h_1} e^{h_1 (c_1 + W_{1\bullet}\mathbf{v})}... \sum_{h_m} e^{h_m (c_m + W_{m\bullet}\mathbf{v})} \\
= & e^{\mathbf{b}'\mathbf{v}} \prod_{j=1}^{m} \left( 1 + e^{c_i + W_{i\bullet}\mathbf{v}} \right)
\label{eq:freeEnergy}
\end{split}
\end{align}

\section*{Appendix}
Following \cite{georges1991expand} lets define
energy as:
\begin{align}
\begin{split}
E = -\sum_{ij} w_{ij}s_i s_j - \sum_i a_i s_i
\end{split}
\end{align}
and introduce the following operator:
\begin{align}
\begin{split}
U \equiv E - \mathbb{E}(E) - \sum_i \frac{\partial \lambda_i (\beta)}{\partial \beta} (s_i - m_i)
\label{eq:Uoperator}
\end{split}
\end{align}
which poses useful property:
$$\mathbb{E}(U) = 0$$
For any other operator $O$ we then have:
\begin{align}
\begin{split}
 \frac{\partial \mathbb{E}(O)}{\partial \beta}  =   \mathbb{E} \left(\frac{\partial O}{\partial \beta} \right) - \mathbb{E}(OU)
 \label{eq:obeta}
\end{split}
\end{align}
Now, the first derivative from the Taylor expansion is:
\begin{align*}
\begin{split}
\frac{\partial (\beta G)}{\partial \beta} = &
\dfrac{\sum_{\mathbf{s}} \exp \left( \beta \sum_{(ij)} w_{ij} s_i s_j +  \beta \sum_i a_is_i+ \sum_i \lambda_i (\beta) (s_i - m_i) \right) \left(\sum_{(ij)} w_{ij} s_i s_j +  \sum_i a_is_i + \sum_i \frac{\partial \lambda_i(\beta)}{\partial \beta} (s_i - m_i) \right) }
{\sum_{\mathbf{s}} \exp \left( \beta \sum_{(ij)} w_{ij} s_i s_j + \sum_i a_is_i + \sum_i \lambda_i (\beta) (s_i - m_i) \right)} \\
 = & \sum_{(ij)} w_{ij} \mathbb{E}(  s_i s_j) +  \sum_i a_i  \mathbb{E}( s_i) + \frac{\partial \lambda_i(\beta)}{\partial \beta} \sum_i \mathbb{E}(
 s_i - m_i ) \\
\end{split}
\end{align*}
In the case of $\beta = 0$ we have:
$$ \left. \frac{\partial (\beta G)}{\partial \beta}\right|_{\beta = 0} = \sum_{(ij)} w_{ij} m_i m_j +  \sum_i a_i m_i $$
Using \ref{eq:obeta} we obtain:
A.1.4:
\begin{align}
\begin{split}
\frac{\partial m_i}{\partial \beta} = 0 = \frac{\partial \mathbb{E}(s_i)}{\partial \beta}  =   \mathbb{E} \left(\frac{\partial s_i}{\partial \beta} \right) - \mathbb{E}(s_iU) =- \mathbb{E}(s_iU)  =- \mathbb{E}(U(s_i - m_i)) 
\end{split}
\end{align}
Thus A.1.5:
\begin{align}
\begin{split}
\frac{\partial U}{\partial \beta}  =  ~&\frac{\partial H}{\partial \beta} - \frac{\partial \mathbb{E}(H)}{\partial \beta} - \sum_i \frac{\partial^2 \lambda_i (\beta)}{\partial \beta^2} (s_i - m_i)\\
= ~& \mathbb{E}(U^2) - \sum_i \frac{\partial^2 \lambda_i (\beta)}{\partial \beta^2} (s_i - m_i)
\end{split}
\end{align}
The second derivative has the form A.1.6:
\begin{align}
\begin{split}
\frac{\partial^2 U}{\partial \beta^2}  =  ~& 2 \mathbb{E}\left(\frac{\partial U}{\partial \beta}U  \right) - \mathbb{E}(U^3) - \sum_i \frac{\partial^3 \lambda_i (\beta)}{\partial \beta^3} (s_i - m_i) \\
= ~& - \mathbb{E}(U^3) - \sum_i \frac{\partial^3 \lambda_i (\beta)}{\partial \beta^3} (s_i - m_i)
\end{split}
\end{align}
Coming back to our expansion of free energy using formulas derived above we now can calculate A.1.7:
\begin{align}
\begin{split}
 \frac{\partial (\beta G)}{\partial \beta} = \mathbb{E}(E) - \sum_i \frac{\partial\lambda_i (\beta)}{\partial \beta}\mathbb{E}(s_i - m_i) = \mathbb{E}(E) 
\end{split}
\end{align}
and A.1.8:
\begin{align}
\begin{split}
 \frac{\partial^2 (\beta G)}{\partial \beta^2} = &~ \mathbb{E}\left( \frac{\partial E}{\partial \beta} \right) - \mathbb{E}(EU) = - \mathbb{E}(U^2)\\
\\
 \frac{\partial^3 (\beta G)}{\partial \beta^3}= &~ - 2\mathbb{E}\left( U \frac{\partial U}{\partial \beta} \right) + \mathbb{E}(U^3)  = \mathbb{E}(U^3)  \\
 \\
  \frac{\partial^4 (\beta G)}{\partial \beta^4}= &~ 3\mathbb{E}\left( U^2 \frac{\partial U}{\partial \beta} \right) - \mathbb{E}(U^4)  = 3\left(\mathbb{E}(U^2)\right)^2 -3\sum_i \frac{\partial^2 \lambda_i (\beta)}{\partial \beta^2}\mathbb{E}(U^2(s_i-m_i))- \mathbb{E}(U^4) 
  \label{eq:higherOrders}
\end{split}
\end{align}
TS was considered around point $\beta =0$. Using derivations from above we obtain again a 'naive' term:
\begin{align}
\begin{split}
\left. \frac{\partial (\beta G)}{\partial \beta} \right|_{\beta =0} = \mathbb{E} _{\beta = 0}(E) = - \sum_{(ij)} w_{ij} m_i m_j - \sum_i a_i m_i = - \frac{1}{2} \sum_i \sum_j w_{ij}m_i m_j - \sum_i a_i m_i
\end{split}
\end{align}
Consider now 
\begin{align}
\begin{split}
\left.\frac{\partial (\beta G)}{\partial m_i \partial \beta}\right|_{\beta = 0} = 
- \sum_{j \neq i} w_{ij} m_j - a_i
\end{split}
\end{align}
On the other hand:
\begin{align}
\begin{split}
\left. \frac{\partial (\beta G)}{\partial m_i \partial \beta}\right|_{\beta = 0} = \left. \frac{\partial (\beta G)}{\partial \beta \partial m_i }\right|_{\beta = 0} = \frac{\partial}{\partial \beta} \mathbb{E} (\lambda_i(\beta)) =  \left. \frac{\partial \lambda_i (\beta)}{\partial \beta}\right|_{\beta =0}
 \label{eq:maxwell}
 \end{split}
\end{align}
Substituting  \ref{eq:maxwell} into  \ref{eq:Uoperator} gives us A.12:
\begin{align}
\begin{split}
U_{\beta = 0} = & -\sum_{(ij)} w_{ij}s_is_j -\sum_i a_i s_i + \frac{1}{2} \sum_i \sum_j w_{ij} m_i m_j + \sum_i a_i m_i + \sum_i \left(\sum_{j \neq i} w_{ij} m_j  + a_i\right)(s_i - m_i ) \\
= & -\sum_{(ij)} w_{ij}s_i s_j - \frac{1}{2}\sum_{(ij)} w_{ij} m_i m_j + \sum_i \sum_{j \neq i} w_{ij} s_i m_j \\
= & - \sum_{(ij)} w_{ij} (s_i - m_i)(s_j- m_j) = - \sum_l w_l y_l
 \end{split}
\end{align}
where $y_l$ stands for the 'link' operator $w_l = w_{ij}$ and $y_l = (s_i -m_i)(s_j-m_j)$ which poses useful properties:

\begin{align}
\begin{split}
\mathbb{E}(y_l)_{\beta =0} = & ~\mathbb{E}(s_i s_j) -m_j\mathbb{E}(s_i) - m_i\mathbb{E}(s_j) + m_i m_j = 0 \\
\mathbb{E}(y_l(s_i-m_i))_{\beta =0} = & ~ m_j - m_j -m_i^2m_j + m_i^2m_j \\
 & ~ - m_i^2m_j + m_i^2m_j + m_i^2m_j - m_i^2m_j \\
 = &~ 0
 \end{split}
\end{align}
Finally, if $k \neq l$ then:
$$\mathbb{E}(y_k y_l)= \mathbb{E}(y_k)\mathbb{E}(y_l)=0$$
while for $k = l$ we have:
\begin{align}
\begin{split}
\mathbb{E}((s_i-m_1)^2(s_j-m_j)^2)= & ~m_im_j - 2m_im_j^2 +m_im_j^2 - 2m_i^2m_j + 4m_1^2m_j^2\\
& - 2m_i^2m_j^2 + m_i^2m_j -2m_i^2m_j^2 + m_i^2m_j^2 \\
= & ~ (m_i -m_i^2)(m_j-m_j^2)
\label{eq:Yoperator}
 \end{split}
\end{align}

Using properties from \ref{eq:Yoperator} in equations \ref{eq:higherOrders} we can derive \ref{eq:taylorExpansion}
A1.13:
\begin{align*}
\begin{split}
\left. \frac{\partial^2 (\beta G)}{\partial \beta^2}\right|_{\beta = 0} = & -\mathbb{E}(U^2)_{\beta =0}\\
= & - \sum_{l_i l_2} w_{l_i}w_{l_2} \mathbb{E}_{\beta = 0} (y_{l_1}y_{l_2} ) \\
= & - \sum_{(i,j)} w_{ij}^2 (m_i-m_i^2)(m_j-m_j^2)
\end{split}
\end{align*}
which yields the TAP-Onsager term.

%Following approach from \ref{eq:maxwell} we have:
%\begin{align*}
%\begin{split}
%\left.\frac{\partial^2 \lambda_i (\beta)}{\partial \beta^2}\right|_{\beta =0} = \left.\frac{\partial^3 (\beta G)}{\partial m_i \partial \beta^2}\right|_{\beta = 0} = 2m_i \sum_{j \neq i} w_{ij}^2(1-m_j^2)
%\end{split}
%\end{align*}

To obtain the next term for the Taylor expansion we need to compute $\mathbb{E}(y_{l_1} y_{l_2} y_{l_3})$ term and by definition the structure of the RBM model doesn't admit triangles in its corresponding factor graphs. Thus, we need to consider on the case when $l_1 = l_2 = l_3$:

\begin{align}
\begin{split}
\mathbb{E}((s_i-m_1)^3(s_j-m_j)^3)= & ~m_i m_j -3m_i m_j^2 +2 m_i m_j^2 + 2m_im_j^3 -3 m_i^2 m_j 
 + 2 m_i^3 m_j \\
 & + 9 m_i^2m_j^2 - 6m_i^3 m_j^2 - 6 m_i^2 m_j^3 + 4m_i^3m_j^3 
 \\
  = &~ 4(m_i - m_i^2)(\frac{1}{2} - m_i)(m_j - m_j^2)(\frac{1}{2} - m_j)
 \end{split}
\end{align}

\begin{align*}
\begin{split}
\left. \frac{\partial^3 (\beta G)}{\partial \beta^3}\right|_{\beta = 0} = \frac{2\beta^3}{3} \sum_{(ij)} w_{ij}^3 (m_i - m_i^2)(\frac{1}{2} - m_i)(m_j - m_j^2)(\frac{1}{2} - m_j)
\end{split}
\end{align*}




\end{document}